{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Text Generation using NLP\n",
    "\n",
    "This notebook is all about me trying to implement the fundamental mechnsm that is used in NLPs the probablity of conecpt of markov chain and building a simple Markov Chain model for text generation using n-grams and transition matrices.\n",
    "\n",
    "## Overview\n",
    "- **Markov Chain**: A stochastic model where the probability of each event depends only on the state attained in the previous event\n",
    "- **N-grams**: Contiguous sequences of n items from a given sample of text\n",
    "- **Transition Matrix**: A matrix that describes the transitions of a Markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import ngrams\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "We start with a sample text and clean it by removing punctuation and converting to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: I love cats. Cats are my favorite animal. I have two cats.\n",
      "Cleaned text: i love cats cats are my favorite animal i have two cats\n"
     ]
    }
   ],
   "source": [
    "text = \"I love cats. Cats are my favorite animal. I have two cats.\"\n",
    "print(f\"Original text: {text}\")\n",
    "text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "text = text.lower()\n",
    "print(f\"Cleaned text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in text: ['i', 'love', 'cats', 'cats', 'are', 'my', 'favorite', 'animal', 'i', 'have', 'two', 'cats']\n",
      "Number of words: 12\n",
      "\n",
      "Generated bigrams (word pairs):\n",
      "[('i', 'love'), ('love', 'cats'), ('cats', 'cats'), ('cats', 'are'), ('are', 'my'), ('my', 'favorite'), ('favorite', 'animal'), ('animal', 'i'), ('i', 'have'), ('have', 'two'), ('two', 'cats')]\n",
      "Number of bigrams: 11\n"
     ]
    }
   ],
   "source": [
    "# Split text into individual words\n",
    "words = text.split()\n",
    "print(f\"Words in text: {words}\")\n",
    "print(f\"Number of words: {len(words)}\")\n",
    "\n",
    "# Generate n-grams (word pairs for n=2)\n",
    "n = 2\n",
    "n_grams = ngrams(words, n)\n",
    "n_grams = list(n_grams)\n",
    "\n",
    "print(f\"\\nGenerated bigrams (word pairs):\")\n",
    "print(n_grams)\n",
    "print(f\"Number of bigrams: {len(n_grams)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in our vocabulary:\n",
      "['have', 'are', 'cats', 'two', 'i', 'my', 'animal', 'favorite', 'love']\n",
      "Vocabulary size: 9 words\n",
      "\n",
      "Word to index mapping:\n",
      "have -> 0\n",
      "are -> 1\n",
      "cats -> 2\n",
      "two -> 3\n",
      "i -> 4\n",
      "my -> 5\n",
      "animal -> 6\n",
      "favorite -> 7\n",
      "love -> 8\n"
     ]
    }
   ],
   "source": [
    "# Get unique words from our text (this will be our vocabulary)\n",
    "unique_words = list(set(words))\n",
    "print(f\"Unique words in our vocabulary:\")\n",
    "print(unique_words)\n",
    "print(f\"Vocabulary size: {len(unique_words)} words\")\n",
    "\n",
    "# Show word to index mapping\n",
    "print(f\"\\nWord to index mapping:\")\n",
    "for i, word in enumerate(unique_words):\n",
    "    print(f\"{word} -> {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Transition Matrix\n",
    "\n",
    "The transition matrix stores the count of how often each word follows another word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix shape: (9, 9)\n",
      "\n",
      "Raw transition matrix (counts):\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Analyzing bigram transitions:\n",
      "('i', 'love') -> count: 1\n",
      "('love', 'cats') -> count: 1\n",
      "('cats', 'cats') -> count: 1\n",
      "('cats', 'are') -> count: 1\n",
      "('are', 'my') -> count: 1\n",
      "('my', 'favorite') -> count: 1\n",
      "('favorite', 'animal') -> count: 1\n",
      "('animal', 'i') -> count: 1\n",
      "('i', 'have') -> count: 1\n",
      "('have', 'two') -> count: 1\n",
      "('two', 'cats') -> count: 1\n"
     ]
    }
   ],
   "source": [
    "# creating a zero matrix with dimensions (vocabulary_size x vocabulary_size)\n",
    "transition_matrix = np.zeros((len(unique_words), len(unique_words)))\n",
    "print(f\"Transition matrix shape: {transition_matrix.shape}\")\n",
    "\n",
    "for i, word in enumerate(unique_words):\n",
    "    for j, next_word in enumerate(unique_words):\n",
    "        count = 0\n",
    "        \n",
    "        for n_gram in n_grams:\n",
    "            if n_gram[0] == word and n_gram[1] == next_word:\n",
    "                count += 1\n",
    "        transition_matrix[i, j] = count\n",
    "\n",
    "print(f\"\\nRaw transition matrix (counts):\")\n",
    "print(transition_matrix)\n",
    "\n",
    "\n",
    "print(f\"\\nAnalyzing bigram transitions:\")\n",
    "for n_gram in n_grams:\n",
    "    word1, word2 = n_gram\n",
    "    i = unique_words.index(word1)\n",
    "    j = unique_words.index(word2)\n",
    "    print(f\"{n_gram} -> count: {int(transition_matrix[i, j])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Transition Matrix\n",
    "\n",
    "Convert counts to probabilities by normalizing each row to sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized transition matrix (probabilities):\n",
      "[[0.  0.  0.  1.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  1.  0.  0.  0. ]\n",
      " [0.  0.5 0.5 0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  1.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0.  0.  0.  0.  0.5]\n",
      " [0.  0.  0.  0.  0.  0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  1.  0.  0. ]\n",
      " [0.  0.  1.  0.  0.  0.  0.  0.  0. ]]\n",
      "\n",
      "Transition probabilities for each word:\n",
      "Word 'have' can be followed by:\n",
      "  -> 'two' with probability 1.00\n",
      "\n",
      "Word 'are' can be followed by:\n",
      "  -> 'my' with probability 1.00\n",
      "\n",
      "Word 'cats' can be followed by:\n",
      "  -> 'are' with probability 0.50\n",
      "  -> 'cats' with probability 0.50\n",
      "\n",
      "Word 'two' can be followed by:\n",
      "  -> 'cats' with probability 1.00\n",
      "\n",
      "Word 'i' can be followed by:\n",
      "  -> 'have' with probability 0.50\n",
      "  -> 'love' with probability 0.50\n",
      "\n",
      "Word 'my' can be followed by:\n",
      "  -> 'favorite' with probability 1.00\n",
      "\n",
      "Word 'animal' can be followed by:\n",
      "  -> 'i' with probability 1.00\n",
      "\n",
      "Word 'favorite' can be followed by:\n",
      "  -> 'animal' with probability 1.00\n",
      "\n",
      "Word 'love' can be followed by:\n",
      "  -> 'cats' with probability 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize the transition matrix to get probabilities\n",
    "transition_matrix = transition_matrix / transition_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "print(f\"Normalized transition matrix (probabilities):\")\n",
    "print(transition_matrix)\n",
    "\n",
    "print(f\"\\nTransition probabilities for each word:\")\n",
    "for i, word in enumerate(unique_words):\n",
    "    print(f\"Word '{word}' can be followed by:\")\n",
    "    for j, next_word in enumerate(unique_words):\n",
    "        prob = transition_matrix[i, j]\n",
    "        if prob > 0:\n",
    "            print(f\"  -> '{next_word}' with probability {prob:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_markov(start_word, num_words=12, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    if start_word not in unique_words:\n",
    "        print(f\"Warning: '{start_word}' not in vocabulary. Available words: {unique_words}\")\n",
    "        return None\n",
    "    \n",
    "    current_word = start_word\n",
    "    generated_text = current_word\n",
    "    \n",
    "    print(f\"Starting with word: '{current_word}'\")\n",
    "    \n",
    "    for step in range(num_words):\n",
    "        \n",
    "        current_word_index = unique_words.index(current_word)\n",
    "        probabilities = transition_matrix[current_word_index]\n",
    "        \n",
    "        # Check if there are any possible next words\n",
    "        if np.sum(probabilities) == 0:\n",
    "            print(f\"No possible next words for '{current_word}'. Stopping generation.\")\n",
    "            break\n",
    "        \n",
    "        # Select next word randomly based on probabilities\n",
    "        next_word_index = np.random.choice(len(unique_words), p=probabilities)\n",
    "        next_word = unique_words[next_word_index]\n",
    "        \n",
    "        print(f\"Step {step + 1}: '{current_word}' -> '{next_word}' (prob: {probabilities[next_word_index]:.2f})\")\n",
    "        \n",
    "        generated_text += \" \" + next_word\n",
    "        \n",
    "        # Set current word to next word for next iteration\n",
    "        current_word = next_word\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: Starting with 'i'\n",
      "Starting with word: 'i'\n",
      "Step 1: 'i' -> 'have' (prob: 0.50)\n",
      "Step 2: 'have' -> 'two' (prob: 1.00)\n",
      "Step 3: 'two' -> 'cats' (prob: 1.00)\n",
      "Step 4: 'cats' -> 'cats' (prob: 0.50)\n",
      "Step 5: 'cats' -> 'are' (prob: 0.50)\n",
      "Step 6: 'are' -> 'my' (prob: 1.00)\n",
      "Step 7: 'my' -> 'favorite' (prob: 1.00)\n",
      "Step 8: 'favorite' -> 'animal' (prob: 1.00)\n",
      "Step 9: 'animal' -> 'i' (prob: 1.00)\n",
      "Step 10: 'i' -> 'love' (prob: 0.50)\n",
      "Step 11: 'love' -> 'cats' (prob: 1.00)\n",
      "Step 12: 'cats' -> 'cats' (prob: 0.50)\n",
      "\n",
      "Generated text: i have two cats cats are my favorite animal i love cats cats\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Start with 'i'\n",
    "print(\"example: Starting with 'i'\")\n",
    "result1 = generate_text_markov('i', num_words=12, seed=42)\n",
    "print(f\"\\nGenerated text: {result1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available words in vocabulary: ['have', 'are', 'cats', 'two', 'i', 'my', 'animal', 'favorite', 'love']\n",
      "Starting with word: 'i'\n",
      "Step 1: 'i' -> 'love' (prob: 0.50)\n",
      "Step 2: 'love' -> 'cats' (prob: 1.00)\n",
      "Step 3: 'cats' -> 'are' (prob: 0.50)\n",
      "Step 4: 'are' -> 'my' (prob: 1.00)\n",
      "Step 5: 'my' -> 'favorite' (prob: 1.00)\n",
      "Step 6: 'favorite' -> 'animal' (prob: 1.00)\n",
      "Step 7: 'animal' -> 'i' (prob: 1.00)\n",
      "Step 8: 'i' -> 'have' (prob: 0.50)\n",
      "Step 9: 'have' -> 'two' (prob: 1.00)\n",
      "Step 10: 'two' -> 'cats' (prob: 1.00)\n",
      "Step 11: 'cats' -> 'are' (prob: 0.50)\n",
      "Step 12: 'are' -> 'my' (prob: 1.00)\n",
      "\n",
      "Final generated text: i love cats are my favorite animal i have two cats are my\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available words in vocabulary: {unique_words}\")\n",
    "current_word = input(\"Enter your initial word to begin from: \")\n",
    "if current_word in unique_words:\n",
    "    result = generate_text_markov(current_word, num_words=12)\n",
    "    print(f\"\\nFinal generated text: {result}\")\n",
    "else:\n",
    "    print(f\"Word '{current_word}' not found in vocabulary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "well this learning notebook was to learn implementation of text generation using Markov chains:\n",
    "\n",
    "### What I Observed:\n",
    "- The model learns word transition patterns from the training text\n",
    "- Words like 'cats' and 'i' have multiple possible next words (probabilistic)\n",
    "- Most other words have deterministic transitions due to limited data\n",
    "- Generated text follows the learned patterns but can create new combinations\n",
    "\n",
    "\n",
    "- Limited training data leads to repetitive patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
